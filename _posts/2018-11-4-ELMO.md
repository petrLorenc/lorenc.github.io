---
layout: post
title: ELMo
description: EMLo is a new State-of-the-art technique to make word embeddings. I was trying to get the main principle and there are few highlights which was very useful for me to understand that.
author: Petr Lorenc
comments: true
---

<a href="https://allennlp.org/elmo">**ELMo**</a> (which stands for **E**mbeddings from **L**anguage **Mo**del) is one of the new State-of-the-art technique to make word embeddings. I was trying to get the main principle and now I present a few highlights which were very useful for me to comprehend that.


<figure class="image" align="middle">
  <a href="{{ site.baseurl }}/images/elmo/pic.png" data-lightbox="CNN over characters" data-title="CNN over characters" data-lightbox="roadtrip">
    <img src="{{ site.baseurl }}/images/elmo/pic.png" alt="CNN over characters" title="CNN over characters"/>
    <figcaption>CNN over characters</figcaption>
  </a>
</figure>

  * **ELMo’s inputs are characters rather than words**. They can thus take advantage of sub-word units to compute meaningful representations even for out-of-vocabulary words (like FastText).
  * ELMo are concatenations of the activations on several layers of the biLMs. **Different layers of a language model encode a different kind of information**


<figure class="image" align="middle">
  <a href="{{ site.baseurl }}/images/elmo/highligts.png" data-lightbox="Base parameters" data-title="Base parameters" data-lightbox="roadtrip">
    <img src="{{ site.baseurl }}/images/elmo/highligts.png" alt="Base parameters" title="Base parameters"/>
    <figcaption>Base parameters of the biggest ELMo model</figcaption>
  </a>
</figure>

Size of char embeddings vocabulary is <a href="https://github.com/allenai/bilm-tf/blob/master/README.md#whats-the-deal-with-n_characters-and-padding">**262 chars**</a> because:

  * char ids 0-255 come from utf-8 encoding bytes
  * assign 256-300 to special chars
    * self.bos_char = 256 \# begin sentence
    * self.eos_char = 257 \# end sentence
    * self.bow_char = 258 \# begin word
    * self.eow_char = 259 \# end word
    * self.pad_char = 260 \# padding

The UnicodeCharsVocabulary that converts token strings to lists of character ids always uses a fixed number of character embeddings of n_characters=261, so always set **n_characters=261 during training.**

However, **for prediction**, we ensure each sentence is fully contained in a single batch, and as a result pad sentences of different lengths **with a special padding id**. This occurs in the Batcher see here. As a result, set n_characters=262 during prediction in the options.json.


<figure class="image" align="middle">
  <a href="{{ site.baseurl }}/images/elmo/youtube.png" data-lightbox="Architecture of ELMo taken from youtube video" data-title="Architecture of ELMo taken from youtube video" data-lightbox="roadtrip">
    <img src="{{ site.baseurl }}/images/elmo/youtube.png" alt="Architecture of ELMo taken from youtube video" title="Architecture of ELMo taken from youtube video"/>
    <figcaption>Architecture of ELMo taken from youtube video</figcaption>
  </a>
</figure>

The screenshot is taken from a <a href="https://youtu.be/9JfGxKkmBc0">video</a>, which is also trying to describe ELMo architecture. I think it is worth to look but I miss there a more detailed part about CNN over chars. On the other side, it nicely describes how the bi-LSTM part works.

The softmax is over <a href="https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt">the vocabulary</a> which is **793 471** tokens big.


<figure class="image" align="middle">
  <a href="{{ site.baseurl }}/images/elmo/youtube2.png" data-lightbox="Architecture of ELMo taken from youtube video" data-title="Architecture of ELMo taken from youtube video" data-lightbox="roadtrip">
    <img src="{{ site.baseurl }}/images/elmo/youtube2.png" alt="Architecture of ELMo taken from youtube video" title="Architecture of ELMo taken from youtube video"/>
    <figcaption>Architecture of ELMo taken from youtube video</figcaption>
  </a>
</figure>

The principle is to train that model on a big corpus and then fine-tuned **multiplying** parameters on task-specific data. The final embedding is a combination of the outputs of all the layers multiplied by their **multiplying** parameters and then average them.


Other notes:

  * **3 GTX 1080 for 10 epochs, taking about two weeks.**
  * The model was trained with a fixed size window of 20 tokens. 
  * The batches were constructed by padding sentences with \<S\> and \<\/S\>, then packing tokens from one or more sentences into each row to fill completely fill each batch. 
    * converts token strings to lists of character ids always uses a fixed number of character embeddings
  * **Partial sentences and the LSTM states** were carried over from batch to batch so that the language model could use information across batches for context, but backpropagation was broken at each batch boundary.

You can look at their results at their <a href="https://allennlp.org/elmo">**pages**</a>


